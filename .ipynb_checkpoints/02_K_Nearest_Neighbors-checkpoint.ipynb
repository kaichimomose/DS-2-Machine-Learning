{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Building a K-Nearest Neighbors Classifier</h1></center>\n",
    "<br>\n",
    "<br>\n",
    "In this tutorial, we'll build our very **_K-Nearest Neighbors_** classifier (KNN for short).  This tutorial will be different from other tutorials in this class in that, unlike most other tutorials where we focus only on using the classifier provided in the `sklearn` library, KNN is simple enough that we can also build one from scratch in this tutorial! We'll still use `sklearn` for things such as such as splitting our data into training and testing sets, and for _scaling_ our data (more on this later), but we'll build the actual algorithm ourselves to get a feel for how it all works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Understanding How KNN Works</h3></center>\n",
    "\n",
    "The KNN algorithm is a classic choice for classification in Machine Learning.  KNN works on the assumption that things that are \"near\" each other are more similar, and therefore more likely to be the same class.  Here's a brief explanation of how KNN actually works under the hood:\n",
    "\n",
    "1.  KNN stores all the data points and labels when `.fit()` is called.  No actual computation is done at this point. \n",
    "<br> \n",
    "<br>\n",
    "2.  When used for predicting the class of an array of objects, the user passes in an array of X_values and a value for K.   \n",
    "<br>\n",
    "3.  KNN calculates the Euclidean distance between the point in question and all other points it stored during the `.fit()` step.   \n",
    "<br> \n",
    "4.  KNN uses these Euclidean distance metrics to select the K nearest points, and looks at what class each is.  \n",
    "<br> \n",
    "5.  The algorithm predicts that the data point in question is whichever class most the majority of it's K nearest neighbors are.  For instance, if `K=5`, and 3 of the neighbors for `X[0]` are class A and 2 are class B, the algorithm will predict that `X[0]` is class A.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this method rather than use the Pythagorean theorem to calculate Euclidean distance manually.  \n",
    "from scipy.spatial import distance\n",
    "\n",
    "class KNN():\n",
    "    \n",
    "    def _euc(self, a, b):\n",
    "        '''Helper function that returns the Euclidean distance individual points a and b '''\n",
    "        return distance.euclidean(a, b)\n",
    "    \n",
    "    def _distance(self, x1, x2):\n",
    "        \"\"\" Calculates the l2 distance between two vectors of the same length.  Loops through each \n",
    "        corresponding row in the vectors and uses the _euc() helper method at each iteration to get the sum of \n",
    "        the distance between all corresponding rows in the two vectors.  \n",
    "        (x1[0] -> x2[0], x1[1] -> x2[1]...x1[n] -> x2[n])\"\"\"\n",
    "        distance = 0\n",
    "        for i in range(len(x1)):\n",
    "            distance += self._euc(x1[i], x2[i])\n",
    "        return distance\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Stores values for X_train and y_train.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test, k):\n",
    "        \"\"\"Predicts the label of the class for each row in X_test, based on the single closest point to each. \n",
    "        The way this is currently written, K is hardcoded to one so the user does not pass in a value for K when \n",
    "        calling it.\n",
    "        \n",
    "        TODO:  Modify this function and the function below so that the user can pass in a value for K, so that \n",
    "        the algorithm makes it's prediction based on the K nearest neighbors (rather than the single closest \n",
    "        neighbor as it is currently written.)  You will need to modify this function AND _closest() to \n",
    "        complete this task.\"\"\"\n",
    "        predicted_classes = []\n",
    "        for point in X_test:\n",
    "            predicted_classes.append(self._closest(point, k))\n",
    "        return predicted_classes\n",
    "        \n",
    "    def _closest(self, row, k):\n",
    "        \"\"\"Modify this function so that it takes a parameter, K, and retrieves the K closest points.  As is, this\n",
    "        function currently only retrieves the labels of the single closest point.  \"\"\"\n",
    "        class_dict = {}\n",
    "        shortest_distances = []\n",
    "        shortest_distances_indexes = []\n",
    "        while len(shortest_distances) < k:\n",
    "            index_of_closest = 0\n",
    "            shortest_distance = self._distance(row, self.X_train[0])\n",
    "            for i in range(1, len(self.X_train)):\n",
    "                current_distance = self._distance(row, self.X_train[i])\n",
    "                if len(shortest_distances) == 0:\n",
    "                    # If this distance is shorter than the shortest distance, make it the new shortest distance.  \n",
    "                    if current_distance < shortest_distance:\n",
    "                        shortest_distance = current_distance\n",
    "                        index_of_closest = i\n",
    "                else:\n",
    "                    if current_distance < shortest_distance and current_distance > shortest_distances[len(shortest_distances) - 1]:\n",
    "                        shortest_distance = current_distance\n",
    "                        index_of_closest = i\n",
    "            shortest_distances_indexes.append(index_of_closest)\n",
    "            shortest_distances.append(shortest_distance)\n",
    "            \n",
    "        print(shortest_distances_indexes)\n",
    "        for index in shortest_distances_indexes:\n",
    "            _class = y_train[index]\n",
    "            if _class not in class_dict:\n",
    "                class_dict[_class] = 1\n",
    "            else:\n",
    "                class_dict[_class] += 1\n",
    "        print(class_dict)\n",
    "        \n",
    "        most_frewuence_class = y_train[0]\n",
    "        largest_number = 0\n",
    "        for key in class_dict:\n",
    "            if largest_number < class_dict[key]:\n",
    "                largest_number = class_dict[key]\n",
    "                most_frequence_class = key\n",
    "\n",
    "        return most_frequence_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Data Preprocessing: Scaling the Data</h3></center>\n",
    "    \n",
    "One quirk of Machine Learning algorithms is that they often do poorly if the scales of the data differ from column to column.  This is especially true for KNN, since the main measure of similarity we're using is Euclidean distance.  For example, let's think about the titanic dataset.  In the age column, a distance of 30 units is a very big deal--this is essentially the distance between children (high survival rate) and adults (medium survival rate), as well as between adults and the elderly (low survival rate).  What about a distance of 30 units in another column, such as ticket price?  With tickets ranging from free to $500+, a difference of 30 units in this column has much less importance.  Herein lies the problem--if we don't scale each column, then the difference in age--which is very important--will likely be drowned out by a column with a much larger scale, such as ticket price.  \n",
    "\n",
    "The solution? **_Scale_** our data to a mean of 0 and a variance of 1.  Since you've already done this sort of data transformation in DS1, we won't spend much time on it here.  Instead, we'll just use the `StandardScaler` object from SKLearn.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1.  Create a `StandardScaler` object and store it in a variable.  \n",
    "2.  Pass in the data to be scaled using the `StandardScaler` object's `.fit()` method.  \n",
    "3.  Call the scaler's `.transform()` method on the data to be scaled (this should be the same data you passed in during the fit step) and store the result it returns in a variable.  You now data that is scaled for use in an algorithm such as KNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# use load_iris() to bind the data to the 'iris' variable. The iris object will contain the data in it's .data \n",
    "#attribute, and the labels for the data in it's .target attribute\n",
    "iris = load_iris()\n",
    "X_vals = iris.data\n",
    "y_vals = iris.target\n",
    "\n",
    "# Create a StandardScaler() Object. \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Call scaler.fit() on the X_vals that will be rescaled.\n",
    "scaler.fit(X_vals)\n",
    "# Bind the newly scaled X_vals to scaled_X_vals by calling scaler.transform() on X_vals.\n",
    "scaled_X_vals = scaler.transform(X_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Using Our Classifier</h3></center>\n",
    "\n",
    "Now that we have created a KNN classifier of our own and scaled our data set, let's use it to make some predictions! \n",
    "\n",
    "Follow the instructions in the comments below to use your KNN classifier to make predictions for the data in X_test.  Once you're done with that, follow the instructions to import sklearn's KNN classifier and get some practice with it! Be sure to use the `accuracy_score` object from `sklearn` to check how well each did.  \n",
    "<br>\n",
    "**_Bonus Challenge:_** Try different values for K and see if you can find the optimal number of neighbors for the most accurate predictions on this data set!\n",
    "\n",
    "### NOTE: NEVER roll your own classifiers in the real world!\n",
    "\n",
    "In this notebook, we created our own KNN classifier to get a feel for how it works. This is great for learning purposes, but nothing you should ever use in production.  Don't build your own classifiers in the wild--it will almost certainly be worse than the highly optimized versions found in SciKit-Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 85, 32, 55, 36]\n",
      "{1: 5}\n",
      "[26, 93, 92, 84, 71]\n",
      "{2: 5}\n",
      "[13, 46, 102, 53, 17]\n",
      "{0: 5}\n",
      "[95, 55, 85, 10, 32]\n",
      "{1: 5}\n",
      "[0, 0, 0, 0, 0]\n",
      "{2: 5}\n",
      "[74, 81, 37, 13, 23]\n",
      "{0: 5}\n",
      "[109, 3, 30, 94, 92]\n",
      "{2: 5}\n",
      "[85, 95, 32, 55, 10]\n",
      "{1: 5}\n",
      "[9, 38, 52, 69, 61]\n",
      "{0: 5}\n",
      "[2, 103, 8, 29, 49]\n",
      "{0: 5}\n",
      "[93, 94, 18, 92, 6]\n",
      "{2: 3, 1: 2}\n",
      "[45, 56, 73, 32, 18]\n",
      "{1: 5}\n",
      "[30, 73, 67, 109, 18]\n",
      "{2: 3, 1: 2}\n",
      "[17, 46, 74, 13, 81]\n",
      "{0: 5}\n",
      "[4, 21, 104, 58, 12]\n",
      "{2: 5}\n",
      "[94, 33, 67, 41, 30]\n",
      "{2: 5}\n",
      "[94, 3, 41, 28, 109]\n",
      "{2: 4, 1: 1}\n",
      "[18, 32, 10, 80, 6]\n",
      "{1: 5}\n",
      "[33, 43, 97, 67, 30]\n",
      "{2: 5}\n",
      "[17, 46, 13, 74, 81]\n",
      "{0: 5}\n",
      "[42, 57, 47, 87, 71]\n",
      "{2: 5}\n",
      "[87, 60, 16, 42, 26]\n",
      "{2: 5}\n",
      "[59, 14, 19, 5, 49]\n",
      "{0: 5}\n",
      "[93, 92, 26, 84, 24]\n",
      "{2: 5}\n",
      "[99, 48, 40, 101, 36]\n",
      "{1: 5}\n",
      "[46, 53, 62, 13, 82]\n",
      "{0: 5}\n",
      "[64, 88, 44, 75, 66]\n",
      "{1: 5}\n",
      "[70, 55, 95, 48, 40]\n",
      "{1: 5}\n",
      "[107, 39, 56, 75, 1]\n",
      "{1: 5}\n",
      "[10, 85, 32, 36, 55]\n",
      "{1: 5}\n",
      "[9, 110, 38, 61, 69]\n",
      "{0: 5}\n",
      "[5, 54, 49, 22, 89]\n",
      "{0: 5}\n",
      "[99, 48, 15, 101, 95]\n",
      "{1: 5}\n",
      "[18, 6, 80, 32, 39]\n",
      "{1: 5}\n",
      "[77, 35, 108, 99, 48]\n",
      "{1: 5}\n",
      "[61, 5, 25, 54, 8]\n",
      "{0: 5}\n",
      "[85, 95, 55, 70, 48]\n",
      "{1: 5}\n",
      "[16, 11, 79, 0, 0]\n",
      "{2: 5}\n",
      "[1, 2, 0, 1, 2, 0, 2, 1, 0, 0, 2, 1, 2, 0, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2]\n",
      "0.9473684210526315 % accurate\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Use train_test_split to split our scaled data into training and testing sets.  Use a test amount of .5.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, random_state=31)\n",
    "#TODO: Create a KNN object using the class you wrote above.  Fit the data and use it to predict labels for X_test.  \n",
    "clf = KNN()\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test, 5)\n",
    "print(predict)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: Use the accuracy_score object to evaluate the quality of your KNN object's predictions. Try passing in different\n",
    "# values for K at prediction time and see which value does best!\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"{} % accurate\".format(accuracy))\n",
    "\n",
    "# TODO: Finally, get some practice with SKLearn's KNN Classifier.  Import the KNeighborsClassifier object from \n",
    "# sklearn.neighbors.  Fit the data to the object and use it to make predictions just as you did with your own KNN above. \n",
    "# Finally, use the accuracy_score object to measure the quality of this classifier's predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
